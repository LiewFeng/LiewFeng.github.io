<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META charset="UFT-8">
  <META name="description" content="Feng Liu's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK rel="stylesheet" type="text/css" href="files/doc.css"  > 
  <TITLE>Feng Liu</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Feng Liu &nbsp;</H1></DIV>
        <H3>Ph.D. candidate</H3>
        <P>Room 260, Academy 2 Building 
        <BR>School of Computer Science and Technology
        <BR>University of Chinese Academy of Sciences
        <BR>Beijing, China, 101408.
        <BR>
        <BR> Email:  
        <A href="mailto:liufeng20@mails.ucas.ac.cn"> liufeng20@mails.ucas.ac.cn</A>; 
        <BR> Github: 
        <A href="https://github.com/LiewFeng">https://github.com/LiewFeng</A>;
        <BR> Google Scholar:
        <A href="https://scholar.google.com/citations?hl=en&user=gIfJkkQAAAAJ">https://scholar.google.com</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/person_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>Biography</H2>
  <P> I am a Ph.D. candidate in <A href="http://lamp.ucas.ac.cn/">LAMP (Learning And Machine Perception Laboratory)</A> in the 
    <A href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</A>, 
    advised by <A href="http://people.ucas.ac.cn/~0064962?language=en">Prof. Fang Wan</A> and <A href="https://people.ucas.ac.cn/~qxye?language=en">Prof. Qixiang Ye</A>. 
    I earned a B.E. from the Harbin Institute of Technology, Shenzhen in June 2020.      
  </P>


  <P>My research interests revolve around Visual Perception, Visual Generation and Multi-modal Understanding.

  <P><b>I am expected to graduate in 2025, and I am actively looking for full-time jobs related to the above topics.</b> </P>

  <H2>Experience</H2>
  <P> Research Intern, 3D Object Detection at Megvii Inc, 2023.04-2024.01      
  </P>
  <P> Research Intern, Video Generation at Alibaba TongYi Vision Intelligence Lab, 2024.07-   
  </P>


  <H2>Publications & Preprints</H2>(* Equal Contribution)
    <table class="pub_table">
    <!-- <tbody> -->

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/TeaCache.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Timestep Embedding Tells: It's Time to Cache for Video Diffusion Model</b>
          <br><b>Feng Liu</b>, Shiwei Zhang, Xiaofeng Wang, Yujie Wei, Haonan Qiu, Yuzhong Zhao, Yingya Zhang, Qixiang Ye, Fang Wan
          <br>2025 IEEE/CVF  Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> 2025)
          <br>
          [<a href="https://arxiv.org/abs/2411.19108">Paper</a>]
          [<a href="https://github.com/LiewFeng/TeaCache">Project</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/DynRefer.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>DynRefer: Delving into Region-level Multi-modality Tasks via Dynamic Resolution</b>
          <br>Yuzhong Zhao*, <b>Feng Liu*</b>, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan
          <br>2025 IEEE/CVF  Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> 2025)
          <br>
          [<a href="https://arxiv.org/abs/2405.16071">Paper</a>]
          [<a href="https://github.com/callsys/DynRefer">Code</a>]
          <br>
        </td>
      </tr>
      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/RayDN.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view 3D Object Detection</b>
          <br><b>Feng Liu</b>, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang Wan, Qixiang Ye, Yanzhao Zhou
          <br>2024 European Conference on Computer Vision (<b>ECCV</b> 2024)
          <br>
          [<a href="https://www.ecva.net/papers/eccv_2024/papers_ECCV/html/6549_ECCV_2024_paper.php">Paper</a>]
          [<a href="https://github.com/LiewFeng/RayDN">Code</a>]
          <br>
        </td>
      </tr>
      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/imTED.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual Object Detection</b>
          <br><b>Feng Liu*</b>, Xiaosong Zhang*, Zhiliang Peng, Zonghao Guo, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br>2023 IEEE/CVF International Conference on Computer Vision (<b>ICCV</b> 2023)
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html">Paper</a>]
          [<a href="https://github.com/LiewFeng/imTED">Code</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/DC.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Domain Contrast for Domain Adaptive Object Detection</b>
          <br><b>Feng Liu</b>, Xiaosong Zhang, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br>2021 IEEE Transactions on Circuits and Systems for Video Technology (<b>T-CSVT</b> 2021)
          <br>
          [<a href="https://ieeexplore.ieee.org/abstract/document/9462093">Paper</a>]
          [<a href="https://github.com/LiewFeng/Domain-Contrast">Code</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/CME.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Beyond Max-Margin: Class Margin Equilibrium for Few-shot Object Detection</b>
          <br>Bohao Li, Boyu Yang, Chang Liu, <b>Feng Liu</b>, Rongrong Ji, Qixiang Ye 
          <br>2021 IEEE/CVF  Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> 2021)
          <br>
          [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.html">Paper</a>]
          [<a href="https://github.com/Bohao-Lee/CME">Code</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/EgoVid.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>EgoVid-5M: A Large-Scale Video-Action Dataset for Egocentric Video Generation</b>
          <br>Xiaofeng Wang, Kang Zhao, <b>Feng Liu</b>, Jiayu Wang, Guosheng Zhao, Xiaoyi Bao, Zheng Zhu, Yingya Zhang, Xingang Wang
          <br>arXiv, 2024
          <br>
          [<a href="https://arxiv.org/pdf/2411.08380">Paper</a>]
          [<a href="https://egovid.github.io/">Project</a>]
          [<a href="https://modelscope.cn/datasets/iic/EgoVid/">Data</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/DreamVideo-2.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>DreamVideo-2: Zero-Shot Subject-Driven Video Customization with Precise Motion Control</b>
          <br>Yujie Wei, Shiwei Zhang, Hangjie Yuan, Xiang Wang, Haonan Qiu, Rui Zhao, Yutong Feng, <b>Feng Liu</b>, Zhizhong Huang, Jiaxin Ye, Yingya Zhang, Hongming Shan
          <br>arXiv, 2024
          <br>
          [<a href="https://arxiv.org/abs/2410.13830">Paper</a>]
          [<a href="https://dreamvideo2.github.io/">Project</a>]
          <br>
        </td>
      </tr>

      </table>

  <br> <br> 
  <H2>Statistics</H2>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5gvvp7wyl47&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
</BODY>
</HTML>
