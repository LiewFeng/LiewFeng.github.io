<!DOCTYPE html>

<HTML>
<HEAD>
  <META content="IE=5.0000" http-equiv="X-UA-Compatible">
  <META charset="UFT-8">
  <META name="description" content="Feng Liu's home page"> 
  <META http-equiv="Content-Type" content="text/html; charset=gb2312">
  <LINK rel="stylesheet" type="text/css" href="files/doc.css"  > 
  <TITLE>Feng Liu</TITLE> 
  <META name="GENERATOR" content="MSHTML 11.00.10570.1001">
</HEAD>


<BODY> 
  <DIV id="layout-content" style="margin-top: 25px;">
  <TABLE>
    <TBODY>
    <TR>
      <TD width="670">
        <DIV id="toptitle">
        <H1>Feng Liu &nbsp;</H1></DIV>
        <H3>Ph.D. candidate</H3>
        <P>Room 260, Academy 2 Building 
        <BR>School of Computer Science and Technology
        <BR>University of Chinese Academy of Sciences
        <BR>Beijing, China, 101408.
        <BR>
        <BR> Email:  
        <A href="mailto:liufeng20@mails.ucas.ac.cn"> liufeng20@mails.ucas.ac.cn</A>; 
        <BR> Github: 
        <A href="https://github.com/LiewFeng">https://github.com/LiewFeng</A>;
        <BR> Google Scholar:
        <A href="https://scholar.google.com/citations?hl=en&user=gIfJkkQAAAAJ">https://scholar.google.com</A>
        <BR><BR></P>
      </TD>
      <TD>
        <IMG width="150" src="files/person_photo.jpg" border="0">
      </TD>
    </TR>
    <TR></TR></TBODY>
  </TABLE>
  <DIV id="layout-content" style="margin-top: 25px;">


  <H2>Biography</H2>
  <P> I am a Ph.D. candidate in <A href="http://lamp.ucas.ac.cn/">LAMP (Learning And Machine Perception Laboratory)</A> in the 
    <A href="http://english.ucas.ac.cn/">University of Chinese Academy of Sciences</A>, 
    advised by <A href="http://people.ucas.ac.cn/~0064962?language=en">Prof. Fang Wan</A> and <A href="https://people.ucas.ac.cn/~qxye?language=en">Prof. Qixiang Ye</A>. 
    I earned a B.E. from the Harbin Institute of Technology, Shenzhen in June 2020.      
  </P>


  <P>I did research about 2D Object Detection and 3D Object Detection (4 paper accepted) before. I did research about Multimodal Large Language Model (1 paper under review) recently.  Now I focus and Text-to-Image/Video Model. 

  <P><b>I am expected to graduate in 2025, and I am actively looking for full-time jobs on Text-to-Image/Video Model(1st choice) or Multimodal Large Language Model(2nd choice).</b> </P>

  <H2>Experience</H2>
  <P> Research Intern, 3D Object Detection, Megvii Inc, 2023.04-2024.01      
  </P>
  <P> Research Intern, Video Generation, Alibaba TongYi Vision Intelligence Lab, 2024.07-   
  </P>


  <H2>Publications & Preprints</H2>(* Equal Contribution)
    <table class="pub_table">
    <!-- <tbody> -->

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/DynRefer.png" class="papericon"></td>
        <td 
          class="pub_td2">Yuzhong Zhao*, <b>Feng Liu*</b>, Yue Liu, Mingxiang Liao, Chen Gong, Qixiang Ye, Fang Wan
          <br>DynRefer: Delving into <b>Region-level Multi-modality Tasks</b> via Dynamic Resolution
          <br>arXiv
          <br>
          [<a href="https://arxiv.org/abs/2405.16071">Paper</a>]
          [<a href="https://github.com/callsys/DynRefer">Code</a>]
          <br>
        </td>
      </tr>
      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/RayDN.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Feng Liu</b>, Tengteng Huang, Qianjing Zhang, Haotian Yao, Chi Zhang, Fang Wan, Qixiang Ye, Yanzhao Zhou
          <br>Ray Denoising: Depth-aware Hard Negative Sampling for Multi-view <b>3D Object Detection</b>
          <br>2024 European Conference on Computer Vision (<b>ECCV</b> 2024)
          <br>
          [<a href="https://arxiv.org/abs/2402.03634">Paper</a>]
          [<a href="https://github.com/LiewFeng/RayDN">Code</a>]
          <br>
        </td>
      </tr>
      
      <tr>
        <td class="pub_td1"><img src="files/PaperFig/imTED.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Feng Liu*</b>, Xiaosong Zhang*, Zhiliang Peng, Zonghao Guo, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br>Integrally Migrating Pre-trained Transformer Encoder-decoders for Visual <b>Object Detection</b>
          <br>2023 IEEE/CVF International Conference on Computer Vision (<b>ICCV</b> 2023)
          <br>
          [<a href="https://openaccess.thecvf.com/content/ICCV2023/html/Liu_Integrally_Migrating_Pre-trained_Transformer_Encoder-decoders_for_Visual_Object_Detection_ICCV_2023_paper.html">Paper</a>]
          [<a href="https://github.com/LiewFeng/imTED">Code</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/DC.png" class="papericon"></td>
        <td 
          class="pub_td2"><b>Feng Liu</b>, Xiaosong Zhang, Fang Wan, Xiangyang Ji, Qixiang Ye
          <br>Domain Contrast for Domain Adaptive <b>Object Detection</b>
          <br>2021 IEEE Transactions on Circuits and Systems for Video Technology (<b>T-CSVT</b> 2021)
          <br>
          [<a href="https://ieeexplore.ieee.org/abstract/document/9462093">Paper</a>]
          [<a href="https://github.com/LiewFeng/Domain-Contrast">Code</a>]
          <br>
        </td>
      </tr>

      <tr>
        <td class="pub_td1"><img src="files/PaperFig/CME.png" class="papericon"></td>
        <td 
          class="pub_td2">Bohao Li, Boyu Yang, Chang Liu, <b>Feng Liu</b>, Rongrong Ji, Qixiang Ye 
          <br>Beyond Max-Margin: Class Margin Equilibrium for Few-shot <b>Object Detection</b>
          <br>2021 IEEE/CVF  Conference on Computer Vision and Pattern Recognition (<b>CVPR</b> 2021)
          <br>
          [<a href="https://openaccess.thecvf.com/content/CVPR2021/html/Li_Beyond_Max-Margin_Class_Margin_Equilibrium_for_Few-Shot_Object_Detection_CVPR_2021_paper.html">Paper</a>]
          [<a href="https://github.com/Bohao-Lee/CME">Code</a>]
          <br>
        </td>
      </tr>
      </table>

  <br> <br> 
  <H2>Statistics</H2>
  <script type="text/javascript" src="//rf.revolvermaps.com/0/0/8.js?i=5gvvp7wyl47&amp;m=0&amp;c=ff0000&amp;cr1=ffffff&amp;f=arial&amp;l=33" async="async"></script>
</BODY>
</HTML>
